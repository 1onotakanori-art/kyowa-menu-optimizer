#!/usr/bin/env python3
"""
Attention ãƒãƒƒãƒ—å¯è¦–åŒ–
éå»ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠãŒã©ã®ã‚ˆã†ã«æ¬¡ã®æ¨å¥¨ã«å½±éŸ¿ã—ã¦ã„ã‚‹ã‹ã‚’åˆ†æ
"""

import numpy as np
import torch
import json
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from datetime import datetime
import sys

sys.path.insert(0, str(Path(__file__).parent))
from menu_encoder import MenuEncoder
from seq2set_transformer import Seq2SetTransformer

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰"""
    menus_dir = Path('/Users/onotakanori/Apps/kyowa-menu-optimizer/menus')
    history_dir = Path('/Users/onotakanori/Apps/kyowa-menu-history/data/history')
    
    all_menus = {}
    menu_to_idx = {}
    idx_to_menu = {}
    idx = 0
    
    menu_files = sorted([f.stem.replace('menus_', '') for f in menus_dir.glob('menus_*.json')])
    history_files = sorted([f.stem for f in history_dir.glob('*.json')])
    common_dates = sorted(set(menu_files) & set(history_files))
    
    for date in common_dates:
        menu_path = menus_dir / f'menus_{date}.json'
        with open(menu_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for menu in data.get('menus', []):
                name = menu['name']
                if name not in menu_to_idx:
                    menu_to_idx[name] = idx
                    idx_to_menu[idx] = name
                    all_menus[name] = menu
                    idx += 1
    
    sequences = []
    for date in common_dates:
        history_path = history_dir / f'{date}.json'
        with open(history_path, 'r', encoding='utf-8') as f:
            history_json = json.load(f)
        
        selected_names = []
        if 'selectedMenus' in history_json and isinstance(history_json['selectedMenus'], list):
            selected_names = [m['name'] for m in history_json['selectedMenus']]
        elif 'eaten' in history_json:
            selected_names = history_json['eaten']
        
        menu_indices = [menu_to_idx[name] for name in selected_names if name in menu_to_idx]
        sequences.append((date, selected_names, menu_indices))
    
    return all_menus, menu_to_idx, idx_to_menu, sequences

def prepare_input(menu_indices, encoder, idx_to_menu, all_menus):
    """å…¥åŠ›ã‚’æº–å‚™"""
    embeddings = []
    for idx in menu_indices:
        menu_name = idx_to_menu.get(idx, '')
        if menu_name in all_menus:
            emb = encoder.encode_menu(all_menus[menu_name])
        else:
            emb = np.zeros(68)
        embeddings.append(emb)
    
    embeddings = np.array(embeddings)
    max_len = 7
    
    if len(embeddings) < max_len:
        pad_len = max_len - len(embeddings)
        embeddings = np.vstack([embeddings, np.zeros((pad_len, 68))])
    
    return torch.from_numpy(embeddings[:max_len]).unsqueeze(0).float()

def analyze_attention(date=None, model_path='ml/seq2set_model_best.pth'):
    """Attention ãƒãƒƒãƒ—ã‚’åˆ†æ"""
    device = torch.device('cpu')
    
    print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\n")
    all_menus, menu_to_idx, idx_to_menu, sequences = load_data()
    num_menus = len(all_menus)
    
    encoder = MenuEncoder()
    encoder.prepare_encoder(list(all_menus.values()))
    
    print("ğŸ§  ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\n")
    model = Seq2SetTransformer(input_dim=68, d_model=128, nhead=4, num_layers=2, num_menus=num_menus)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    
    # å¯¾è±¡æ—¥ã®æ±ºå®š
    if date is None:
        target_idx = len(sequences) - 1
    else:
        target_idx = None
        for i, (d, _, _) in enumerate(sequences):
            if d == date:
                target_idx = i
                break
    
    if target_idx is None:
        print(f"âŒ æ—¥ä»˜ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    target_date = sequences[target_idx][0]
    target_names = sequences[target_idx][1]
    target_indices = sequences[target_idx][2]
    
    print(f"ğŸ“… åˆ†æå¯¾è±¡æ—¥: {target_date}")
    print(f"é¸æŠãƒ¡ãƒ‹ãƒ¥ãƒ¼: {', '.join(target_names[:3])}{'...' if len(target_names) > 3 else ''}\n")
    
    # éå»7æ—¥é–“ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’åé›†
    past_menus = []
    past_dates = []
    
    for i in range(max(0, target_idx - 7), target_idx):
        past_menus.extend(sequences[i][1])
        past_dates.append(sequences[i][0])
    
    if not past_menus:
        print("âš ï¸ éå»ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        return
    
    # å…¥åŠ›ã‚’æº–å‚™
    input_embedding = prepare_input(sequences[target_idx - 1][2] if target_idx > 0 else [], 
                                    encoder, idx_to_menu, all_menus)
    input_embedding = input_embedding.to(device)
    
    # Forward pass with attention tracking
    with torch.no_grad():
        # ãƒ¢ãƒ‡ãƒ«ã‚’é€šã™å‰ã«ã€å…¥åŠ›æŠ•å½±å¾Œã®å€¤ã‚’å–å¾—
        x = model.input_projection(input_embedding)  # (1, 7, 128)
        
        # Attention ã‚’å±¤ã”ã¨ã«å–å¾—
        layer_attentions = []
        for encoder_layer in model.encoder_layers:
            x, attn = encoder_layer(x)
            layer_attentions.append(attn)
        
        scores, _ = model(input_embedding)
    
    # æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆä¸Šä½ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼‰
    top_scores, top_indices = torch.topk(scores[0], k=5)
    top_indices = top_indices.cpu().numpy()
    top_scores = top_scores.cpu().numpy()
    
    print("=" * 70)
    print("ğŸ¯ æ¨å¥¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ (ãƒˆãƒƒãƒ—5)")
    print("=" * 70 + "\n")
    
    for rank, idx in enumerate(top_indices, 1):
        menu_name = idx_to_menu[int(idx)]
        score = top_scores[rank - 1]
        print(f"{rank}. {menu_name:<40} (ã‚¹ã‚³ã‚¢: {score:.3f})")
    
    # Attentionåˆ†æ
    print("\n" + "=" * 70)
    print("ğŸ§  Attention åˆ†æ (éå»ã¸ã®æ³¨è¦–åº¦)")
    print("=" * 70 + "\n")
    
    print(f"ç›´è¿‘7æ—¥ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼:\n")
    for i, (d, menus) in enumerate(zip(past_dates, 
                                        [sequences[j][1] for j in range(max(0, target_idx - 7), target_idx)])):
        print(f"  {d}: {', '.join(menus[:2])}{'...' if len(menus) > 2 else ''}")
    
    # Attention weights ã®çµ±è¨ˆ
    print("\n\nğŸ’¡ Attention ãƒ˜ãƒƒãƒ‰ã®ç‰¹æ€§åˆ†æ:\n")
    
    for layer_idx, attn_weights in enumerate(layer_attentions):
        print(f"ã€Transformer Layer {layer_idx + 1}ã€‘")
        
        # attn_weights: (batch, num_heads, query_len, key_len)
        # query_len=key_len=7, num_heads=4
        
        # å¹³å‡Attention weights ã‚’è¨ˆç®—
        avg_attn = attn_weights[0].mean(dim=0).cpu().numpy()  # (7, 7)
        
        # å¯¾è§’ç·šæˆåˆ†ï¼ˆè‡ªå·±Attentionï¼‰
        diag_attn = np.diag(avg_attn)
        
        print(f"  è‡ªå·±æ³¨è¦–åº¦ (å¯¾è§’ç·šæˆåˆ†):")
        for pos, val in enumerate(diag_attn):
            if pos < target_idx % 7:
                print(f"    ä½ç½®{pos}: {val:.3f}")
        
        # æœ€ã‚‚å¼·ã„æ¥ç¶š
        # å¯¾è§’ç·šä»¥å¤–ã®æœ€å¤§å€¤ã‚’å–å¾—
        off_diag = avg_attn.copy()
        np.fill_diagonal(off_diag, -1)
        max_idx = np.unravel_index(np.argmax(off_diag), off_diag.shape)
        max_val = off_diag[max_idx]
        
        if max_val > 0:
            print(f"  æœ€å¼·æ¥ç¶š: ä½ç½®{max_idx[1]} â†’ ä½ç½®{max_idx[0]} ({max_val:.3f})")
        
        print()
    
    # æ¨å¥¨ç†ç”±ã®ç”Ÿæˆï¼ˆAttention ã«åŸºã¥ãï¼‰
    print("=" * 70)
    print("ğŸ“Š æ¨å¥¨ç†ç”± (Attention ãƒ™ãƒ¼ã‚¹)")
    print("=" * 70 + "\n")
    
    if len(layer_attentions) > 0:
        avg_attn = layer_attentions[-1][0].mean(dim=0).cpu().numpy()
        
        # æœ€å¾Œã®ä½ç½®ï¼ˆæ¬¡ã®æ¨å¥¨å¯¾è±¡ï¼‰ã«å¯¾ã™ã‚‹æ³¨è¦–åº¦
        if avg_attn.shape[0] > 0:
            final_attn = avg_attn[-1] if avg_attn.shape[0] > 1 else avg_attn[0]
            
            # æ³¨è¦–åº¦ãŒé«˜ã„éå»ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’ç‰¹å®š
            important_positions = np.argsort(final_attn)[-3:][::-1]
            
            print("æ¬¡ã®æ¨å¥¨ã«æœ€ã‚‚å½±éŸ¿ã—ãŸéå»ãƒ¡ãƒ‹ãƒ¥ãƒ¼:\n")
            for rank, pos in enumerate(important_positions, 1):
                if pos < len(past_dates):
                    past_date = past_dates[pos]
                    past_menus_at_date = sequences[target_idx - 7 + pos][1]
                    attn_val = final_attn[pos]
                    
                    print(f"  {rank}. {past_date}: {', '.join(past_menus_at_date[:2])}")
                    print(f"     æ³¨è¦–åº¦: {attn_val:.3f}")
    
    print("\n" + "=" * 70)

if __name__ == '__main__':
    date = sys.argv[1] if len(sys.argv) > 1 else None
    analyze_attention(date)
