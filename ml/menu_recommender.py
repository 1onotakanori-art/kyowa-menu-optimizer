#!/usr/bin/env python3
"""
Kyowa Menu Recommender - æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 

å­¦ç¿’å¯¾è±¡ï¼š
1. æ „é¤Šç´ ãƒ™ãƒ¼ã‚¹: PFCçµ¶å¯¾å€¤ã€PFCãƒãƒ©ãƒ³ã‚¹ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€é‡èœé‡é‡ã€é£½å’Œè„‚è‚ªé…¸
2. ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹: ãƒ¡ãƒ‹ãƒ¥ãƒ¼åã«å«ã¾ã‚Œã‚‹å˜èª
3. ãƒ¡ãƒ‹ãƒ¥ãƒ¼é–“é–¢ä¿‚: å…±èµ·ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚«ãƒ†ã‚´ãƒªé–“ã®é–¢ä¿‚
4. è² ä¾‹å­¦ç¿’: é¸ã°ã‚Œãªã‹ã£ãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç‰¹å¾´
"""

import json
import re
import numpy as np
import pandas as pd
from collections import Counter, defaultdict
from sklearn.model_selection import cross_val_score, LeaveOneGroupOut
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
import warnings
warnings.filterwarnings('ignore')


class MenuFeatureExtractor:
    """ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.word_counter = Counter()
        self.word_to_idx = {}
        self.scaler = StandardScaler()
        
    def extract_words(self, menu_name):
        """ãƒ¡ãƒ‹ãƒ¥ãƒ¼åã‹ã‚‰å˜èªã‚’æŠ½å‡º"""
        # ã‚«ã‚¿ã‚«ãƒŠã€ã²ã‚‰ãŒãªã€æ¼¢å­—ã€è‹±æ•°å­—ã§åˆ†å‰²
        words = re.findall(r'[ã‚¡-ãƒ³ãƒ¼]+|[ã-ã‚“ãƒ¼]+|[ä¸€-é¾¯]+|[a-zA-Z]+|\d+', menu_name)
        # çŸ­ã™ãã‚‹å˜èªã‚’é™¤å¤–ï¼ˆ1æ–‡å­—ã®ã‚«ã‚¿ã‚«ãƒŠãƒ»ã²ã‚‰ãŒãªã¯é™¤å¤–ï¼‰
        words = [w for w in words if len(w) > 1 or re.match(r'[ä¸€-é¾¯]', w)]
        return words
    
    def build_vocabulary(self, all_menus):
        """å…¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰èªå½™ã‚’æ§‹ç¯‰"""
        for menu in all_menus:
            words = self.extract_words(menu['name'])
            self.word_counter.update(words)
        
        # å‡ºç¾å›æ•°2å›ä»¥ä¸Šã®å˜èªã®ã¿æ¡ç”¨
        frequent_words = [w for w, c in self.word_counter.most_common() if c >= 2]
        self.word_to_idx = {w: i for i, w in enumerate(frequent_words)}
        print(f"ğŸ“š èªå½™ã‚µã‚¤ã‚º: {len(self.word_to_idx)} å˜èª")
        return frequent_words
    
    def extract_nutrition_features(self, nutrition):
        """æ „é¤Šç´ ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        # åŸºæœ¬æ „é¤Šç´ 
        energy = nutrition.get('ã‚¨ãƒãƒ«ã‚®ãƒ¼', 0) or 0
        protein = nutrition.get('ãŸã‚“ã±ãè³ª', 0) or 0  # P
        fat = nutrition.get('è„‚è³ª', 0) or 0  # F
        carb = nutrition.get('ç‚­æ°´åŒ–ç‰©', 0) or 0  # C
        saturated_fat = nutrition.get('é£½å’Œè„‚è‚ªé…¸', 0) or 0
        salt = nutrition.get('é£Ÿå¡©ç›¸å½“é‡', 0) or 0
        vegetable = nutrition.get('é‡èœé‡é‡', 0) or 0
        
        # PFCãƒãƒ©ãƒ³ã‚¹ï¼ˆã‚«ãƒ­ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ã®æ¯”ç‡ï¼‰
        pfc_total = protein * 4 + fat * 9 + carb * 4
        if pfc_total > 0:
            p_ratio = (protein * 4) / pfc_total
            f_ratio = (fat * 9) / pfc_total
            c_ratio = (carb * 4) / pfc_total
        else:
            p_ratio = f_ratio = c_ratio = 0
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼å¯†åº¦ï¼ˆé‡èœé‡é‡ã‚ãŸã‚Šï¼‰
        energy_density = energy / (vegetable + 1)  # 0é™¤ç®—é˜²æ­¢
        
        # ãŸã‚“ã±ãè³ªåŠ¹ç‡ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚ãŸã‚Šï¼‰
        protein_efficiency = protein / (energy + 1) * 100
        
        return {
            'energy': energy,
            'protein': protein,
            'fat': fat,
            'carb': carb,
            'saturated_fat': saturated_fat,
            'salt': salt,
            'vegetable': vegetable,
            'p_ratio': p_ratio,
            'f_ratio': f_ratio,
            'c_ratio': c_ratio,
            'energy_density': energy_density,
            'protein_efficiency': protein_efficiency,
            'pfc_total': pfc_total,
        }
    
    def extract_text_features(self, menu_name):
        """ãƒ¡ãƒ‹ãƒ¥ãƒ¼åã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        words = self.extract_words(menu_name)
        features = np.zeros(len(self.word_to_idx))
        for word in words:
            if word in self.word_to_idx:
                features[self.word_to_idx[word]] = 1
        return features
    
    def extract_category_features(self, menu_name):
        """ãƒ¡ãƒ‹ãƒ¥ãƒ¼åã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        categories = {
            'is_rice': bool(re.search(r'ãƒ©ã‚¤ã‚¹|ã”é£¯|ã”ã¯ã‚“|ä¸¼|ç‚’é£¯|ãƒãƒ£ãƒ¼ãƒãƒ³', menu_name)),
            'is_noodle': bool(re.search(r'éºº|ãƒ©ãƒ¼ãƒ¡ãƒ³|ãã°|è•éº¦|ã†ã©ã‚“|ãƒ‘ã‚¹ã‚¿', menu_name)),
            'is_meat': bool(re.search(r'è‚‰|ãƒã‚­ãƒ³|ãƒãƒ¼ã‚¯|ãƒ“ãƒ¼ãƒ•|é¶|è±š|ç‰›|ãƒãƒ³ãƒãƒ¼ã‚°|ã‚«ãƒ„', menu_name)),
            'is_fish': bool(re.search(r'é­š|ã‚µãƒ¼ãƒ¢ãƒ³|é¯–|é®­|ã‚¨ãƒ“|æµ·è€|ã‚¤ã‚«|ç™½èº«', menu_name)),
            'is_vegetable': bool(re.search(r'ã‚µãƒ©ãƒ€|é‡èœ|ã‚­ãƒ£ãƒ™ãƒ„|ãƒ¬ã‚¿ã‚¹|ãƒ–ãƒ­ãƒƒã‚³ãƒªãƒ¼', menu_name)),
            'is_soup': bool(re.search(r'æ±|ã‚¹ãƒ¼ãƒ—|å‘³å™Œ|ã¿ã', menu_name)),
            'is_fried': bool(re.search(r'æšã’|ãƒ•ãƒ©ã‚¤|ã‚«ãƒ„|å¤©ã·ã‚‰|å”æšã’', menu_name)),
            'is_healthy': bool(re.search(r'å¥åº·|ãƒ˜ãƒ«ã‚·ãƒ¼|ãŸã‚“ã±ãè³ª|é£Ÿç‰©ç¹Šç¶­|é‡èœãŸã£ã·ã‚Š', menu_name)),
            'is_mini': bool(re.search(r'ãƒŸãƒ‹|å°|ãƒãƒ¼ãƒ•', menu_name)),
            'is_curry': bool(re.search(r'ã‚«ãƒ¬ãƒ¼', menu_name)),
            'is_egg': bool(re.search(r'åµ|ç‰å­|ãŸã¾ã”|ã‚ªãƒ ãƒ¬ãƒ„', menu_name)),
            'is_tofu': bool(re.search(r'è±†è…|å†·å¥´|ç´è±†|å¤§è±†', menu_name)),
            'is_dessert': bool(re.search(r'ãƒ—ãƒªãƒ³|ã‚±ãƒ¼ã‚­|ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆ|ãƒ‡ã‚¶ãƒ¼ãƒˆ|ãƒ•ãƒ«ãƒ¼ãƒ„|ãƒãƒŠãƒŠ', menu_name)),
        }
        return categories


class CooccurrenceAnalyzer:
    """ãƒ¡ãƒ‹ãƒ¥ãƒ¼é–“ã®å…±èµ·é–¢ä¿‚ã‚’åˆ†æã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.cooccurrence_matrix = {}
        self.menu_selection_count = Counter()
        self.category_cooccurrence = {}
        
    def analyze(self, training_data, feature_extractor):
        """å…±èµ·é–¢ä¿‚ã‚’åˆ†æ"""
        for day_data in training_data:
            selected_menus = [m for m in day_data['allMenus'] if m['selected']]
            
            # ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠå›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            for menu in selected_menus:
                self.menu_selection_count[menu['name']] += 1
            
            # ãƒ¡ãƒ‹ãƒ¥ãƒ¼é–“ã®å…±èµ·
            for i, menu1 in enumerate(selected_menus):
                for menu2 in selected_menus[i+1:]:
                    # å…±èµ·è¡Œåˆ—ã‚’æ›´æ–°
                    if menu1['name'] not in self.cooccurrence_matrix:
                        self.cooccurrence_matrix[menu1['name']] = {}
                    if menu2['name'] not in self.cooccurrence_matrix:
                        self.cooccurrence_matrix[menu2['name']] = {}
                    
                    self.cooccurrence_matrix[menu1['name']][menu2['name']] = \
                        self.cooccurrence_matrix[menu1['name']].get(menu2['name'], 0) + 1
                    self.cooccurrence_matrix[menu2['name']][menu1['name']] = \
                        self.cooccurrence_matrix[menu2['name']].get(menu1['name'], 0) + 1
                    
                    # ã‚«ãƒ†ã‚´ãƒªé–“ã®å…±èµ·
                    cat1 = feature_extractor.extract_category_features(menu1['name'])
                    cat2 = feature_extractor.extract_category_features(menu2['name'])
                    for c1, v1 in cat1.items():
                        if v1:
                            if c1 not in self.category_cooccurrence:
                                self.category_cooccurrence[c1] = {}
                            for c2, v2 in cat2.items():
                                if v2:
                                    self.category_cooccurrence[c1][c2] = \
                                        self.category_cooccurrence[c1].get(c2, 0) + 1
        
        print(f"ğŸ“Š ã‚ˆãé¸ã°ã‚Œã‚‹ãƒ¡ãƒ‹ãƒ¥ãƒ¼ TOP 10:")
        for name, count in self.menu_selection_count.most_common(10):
            print(f"   {count}å›: {name}")
        
        print(f"\nğŸ“Š ã‚ˆãä¸€ç·’ã«é¸ã°ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›:")
        category_pairs = []
        for c1, c2_dict in self.category_cooccurrence.items():
            for c2, count in c2_dict.items():
                if c1 < c2:  # é‡è¤‡æ’é™¤
                    category_pairs.append((c1, c2, count))
        category_pairs.sort(key=lambda x: -x[2])
        for c1, c2, count in category_pairs[:5]:
            print(f"   {count}å›: {c1} + {c2}")
    
    def get_cooccurrence_score(self, menu_name, selected_menus):
        """é¸æŠæ¸ˆã¿ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã¨ã®å…±èµ·ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0
        if menu_name in self.cooccurrence_matrix:
            for selected in selected_menus:
                score += self.cooccurrence_matrix[menu_name].get(selected, 0)
        return score


class MenuRecommender:
    """ãƒ¡ãƒ‹ãƒ¥ãƒ¼æ¨è–¦ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self):
        self.feature_extractor = MenuFeatureExtractor()
        self.cooccurrence_analyzer = CooccurrenceAnalyzer()
        self.models = {}
        self.best_model = None
        self.best_model_name = None
        
    def load_data(self, data_path='data/training_data.json'):
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        with open(data_path, 'r', encoding='utf-8') as f:
            self.training_data = json.load(f)
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.training_data)}æ—¥åˆ†")
        return self.training_data
    
    def prepare_features(self):
        """ç‰¹å¾´é‡ã‚’æº–å‚™"""
        print("\nğŸ”§ ç‰¹å¾´é‡æº–å‚™ä¸­...")
        
        # å…¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’åé›†
        all_menus = []
        for day_data in self.training_data:
            all_menus.extend(day_data['allMenus'])
        
        # èªå½™æ§‹ç¯‰
        self.feature_extractor.build_vocabulary(all_menus)
        
        # å…±èµ·åˆ†æ
        print("\nğŸ“ˆ å…±èµ·åˆ†æä¸­...")
        self.cooccurrence_analyzer.analyze(self.training_data, self.feature_extractor)
        
        # ç‰¹å¾´é‡è¡Œåˆ—ã¨ãƒ©ãƒ™ãƒ«ã‚’æ§‹ç¯‰
        X_list = []
        y_list = []
        groups = []  # æ—¥ä»˜ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆLeave-One-Day-Outç”¨ï¼‰
        menu_names = []
        
        for day_idx, day_data in enumerate(self.training_data):
            selected_names = [m['name'] for m in day_data['allMenus'] if m['selected']]
            
            for menu in day_data['allMenus']:
                # æ „é¤Šç´ ç‰¹å¾´é‡
                nutrition_features = self.feature_extractor.extract_nutrition_features(
                    menu.get('nutrition', {})
                )
                
                # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡
                text_features = self.feature_extractor.extract_text_features(menu['name'])
                
                # ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡
                category_features = self.feature_extractor.extract_category_features(menu['name'])
                
                # å…±èµ·ã‚¹ã‚³ã‚¢ï¼ˆç¾åœ¨ã®æ—¥ã®ä»–ã®é¸æŠãƒ¡ãƒ‹ãƒ¥ãƒ¼ã¨ã®ï¼‰
                other_selected = [n for n in selected_names if n != menu['name']]
                cooccurrence_score = self.cooccurrence_analyzer.get_cooccurrence_score(
                    menu['name'], other_selected
                )
                
                # éå»ã®é¸æŠé »åº¦
                selection_frequency = self.cooccurrence_analyzer.menu_selection_count.get(
                    menu['name'], 0
                ) / max(len(self.training_data), 1)
                
                # ç‰¹å¾´é‡ã‚’çµåˆ
                features = list(nutrition_features.values())
                features.extend(text_features)
                features.extend([int(v) for v in category_features.values()])
                features.append(cooccurrence_score)
                features.append(selection_frequency)
                
                X_list.append(features)
                y_list.append(1 if menu['selected'] else 0)
                groups.append(day_idx)
                menu_names.append(menu['name'])
        
        self.X = np.array(X_list)
        self.y = np.array(y_list)
        self.groups = np.array(groups)
        self.menu_names = menu_names
        
        # ç‰¹å¾´é‡åã‚’ä¿å­˜
        nutrition_feature_names = list(self.feature_extractor.extract_nutrition_features({}).keys())
        text_feature_names = [f'word_{w}' for w in self.feature_extractor.word_to_idx.keys()]
        category_feature_names = list(self.feature_extractor.extract_category_features('').keys())
        
        self.feature_names = (
            nutrition_feature_names + 
            text_feature_names + 
            category_feature_names + 
            ['cooccurrence_score', 'selection_frequency']
        )
        
        print(f"\nâœ… ç‰¹å¾´é‡æº–å‚™å®Œäº†:")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(self.X)}")
        print(f"   ç‰¹å¾´é‡æ•°: {len(self.feature_names)}")
        print(f"   æ­£ä¾‹ï¼ˆé¸æŠï¼‰: {sum(self.y)} ({sum(self.y)/len(self.y)*100:.1f}%)")
        print(f"   è² ä¾‹ï¼ˆéé¸æŠï¼‰: {len(self.y)-sum(self.y)} ({(len(self.y)-sum(self.y))/len(self.y)*100:.1f}%)")
        
        return self.X, self.y
    
    def train_models(self):
        """è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—æ¯”è¼ƒ"""
        print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
        X_scaled = self.scaler.fit_transform(self.X) if hasattr(self, 'scaler') else StandardScaler().fit_transform(self.X)
        self.scaler = StandardScaler().fit(self.X)
        X_scaled = self.scaler.transform(self.X)
        
        # ãƒ¢ãƒ‡ãƒ«å®šç¾©
        models = {
            'LogisticRegression': LogisticRegression(
                class_weight='balanced', 
                max_iter=1000,
                random_state=42
            ),
            'RandomForest': RandomForestClassifier(
                n_estimators=100, 
                class_weight='balanced',
                max_depth=10,
                random_state=42
            ),
            'GradientBoosting': GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                random_state=42
            ),
        }
        
        # Leave-One-Day-Out Cross Validation
        logo = LeaveOneGroupOut()
        results = {}
        
        print("\nğŸ“Š Leave-One-Day-Out ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ:")
        print("-" * 60)
        
        best_score = 0
        for name, model in models.items():
            try:
                # AUC-ROCã‚¹ã‚³ã‚¢ã§è©•ä¾¡
                scores = cross_val_score(
                    model, X_scaled, self.y, 
                    cv=logo, groups=self.groups,
                    scoring='roc_auc'
                )
                mean_score = scores.mean()
                std_score = scores.std()
                results[name] = {'mean': mean_score, 'std': std_score, 'scores': scores}
                
                print(f"{name:20s}: AUC-ROC = {mean_score:.4f} (+/- {std_score:.4f})")
                
                if mean_score > best_score:
                    best_score = mean_score
                    self.best_model_name = name
                    
            except Exception as e:
                print(f"{name:20s}: ã‚¨ãƒ©ãƒ¼ - {e}")
        
        print("-" * 60)
        print(f"âœ… æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {self.best_model_name} (AUC-ROC = {best_score:.4f})")
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’
        self.best_model = models[self.best_model_name]
        self.best_model.fit(X_scaled, self.y)
        self.models = {name: model.fit(X_scaled, self.y) for name, model in models.items()}
        
        return results
    
    def analyze_feature_importance(self):
        """ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’åˆ†æ"""
        print("\nğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ:")
        print("-" * 60)
        
        if hasattr(self.best_model, 'feature_importances_'):
            importances = self.best_model.feature_importances_
        elif hasattr(self.best_model, 'coef_'):
            importances = np.abs(self.best_model.coef_[0])
        else:
            print("ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ã§ãã¾ã›ã‚“")
            return
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        indices = np.argsort(importances)[::-1]
        
        print("TOP 20 é‡è¦ãªç‰¹å¾´é‡:")
        for i in range(min(20, len(indices))):
            idx = indices[i]
            if idx < len(self.feature_names):
                print(f"  {i+1:2d}. {self.feature_names[idx]:30s}: {importances[idx]:.4f}")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é‡è¦åº¦é›†è¨ˆ
        print("\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦åº¦:")
        nutrition_end = 13  # æ „é¤Šç´ ç‰¹å¾´é‡ã®æ•°
        text_end = nutrition_end + len(self.feature_extractor.word_to_idx)
        
        nutrition_importance = np.sum(importances[:nutrition_end])
        text_importance = np.sum(importances[nutrition_end:text_end])
        category_importance = np.sum(importances[text_end:-2])
        other_importance = np.sum(importances[-2:])
        total = nutrition_importance + text_importance + category_importance + other_importance
        
        print(f"  æ „é¤Šç´ ç‰¹å¾´é‡: {nutrition_importance/total*100:.1f}%")
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡: {text_importance/total*100:.1f}%")
        print(f"  ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡: {category_importance/total*100:.1f}%")
        print(f"  ãã®ä»–ï¼ˆå…±èµ·ãƒ»é »åº¦ï¼‰: {other_importance/total*100:.1f}%")
    
    def predict(self, menus, already_selected=None):
        """ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒªã‚¹ãƒˆã«å¯¾ã—ã¦æ¨è–¦ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬"""
        if already_selected is None:
            already_selected = []
        
        X_pred = []
        for menu in menus:
            # æ „é¤Šç´ ç‰¹å¾´é‡
            nutrition_features = self.feature_extractor.extract_nutrition_features(
                menu.get('nutrition', {})
            )
            
            # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡
            text_features = self.feature_extractor.extract_text_features(menu['name'])
            
            # ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡
            category_features = self.feature_extractor.extract_category_features(menu['name'])
            
            # å…±èµ·ã‚¹ã‚³ã‚¢
            cooccurrence_score = self.cooccurrence_analyzer.get_cooccurrence_score(
                menu['name'], already_selected
            )
            
            # é¸æŠé »åº¦
            selection_frequency = self.cooccurrence_analyzer.menu_selection_count.get(
                menu['name'], 0
            ) / max(len(self.training_data), 1)
            
            # ç‰¹å¾´é‡ã‚’çµåˆ
            features = list(nutrition_features.values())
            features.extend(text_features)
            features.extend([int(v) for v in category_features.values()])
            features.append(cooccurrence_score)
            features.append(selection_frequency)
            
            X_pred.append(features)
        
        X_pred = np.array(X_pred)
        X_pred_scaled = self.scaler.transform(X_pred)
        
        # ç¢ºç‡ã‚’äºˆæ¸¬
        probabilities = self.best_model.predict_proba(X_pred_scaled)[:, 1]
        
        # çµæœã‚’æ•´ç†
        results = []
        for i, menu in enumerate(menus):
            results.append({
                'name': menu['name'],
                'score': float(probabilities[i]),
                'nutrition': menu.get('nutrition', {})
            })
        
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: -x['score'])
        return results
    
    def save_model(self, path='model'):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        import pickle
        import os
        
        os.makedirs(path, exist_ok=True)
        
        model_data = {
            'best_model': self.best_model,
            'best_model_name': self.best_model_name,
            'scaler': self.scaler,
            'feature_extractor': self.feature_extractor,
            'cooccurrence_analyzer': self.cooccurrence_analyzer,
            'feature_names': self.feature_names
        }
        
        with open(f'{path}/menu_recommender.pkl', 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"\nâœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {path}/menu_recommender.pkl")
    
    @classmethod
    def load_model(cls, path='model/menu_recommender.pkl'):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        import pickle
        
        with open(path, 'rb') as f:
            model_data = pickle.load(f)
        
        recommender = cls()
        recommender.best_model = model_data['best_model']
        recommender.best_model_name = model_data['best_model_name']
        recommender.scaler = model_data['scaler']
        recommender.feature_extractor = model_data['feature_extractor']
        recommender.cooccurrence_analyzer = model_data['cooccurrence_analyzer']
        recommender.feature_names = model_data['feature_names']
        recommender.training_data = []  # äºˆæ¸¬æ™‚ã¯ä¸è¦
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {path}")
        return recommender


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ½ï¸  Kyowa Menu Recommender - å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    recommender = MenuRecommender()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    recommender.load_data()
    
    # ç‰¹å¾´é‡æº–å‚™
    recommender.prepare_features()
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    recommender.train_models()
    
    # ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
    recommender.analyze_feature_importance()
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    recommender.save_model()
    
    # ãƒ†ã‚¹ãƒˆäºˆæ¸¬
    print("\n" + "=" * 60)
    print("ğŸ§ª ãƒ†ã‚¹ãƒˆäºˆæ¸¬ï¼ˆæœ€æ–°æ—¥ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼‰")
    print("=" * 60)
    
    if recommender.training_data:
        test_day = recommender.training_data[-1]
        test_menus = test_day['allMenus']
        
        # å®Ÿéš›ã«é¸ã°ã‚ŒãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼
        actual_selected = [m['name'] for m in test_menus if m['selected']]
        print(f"\nå®Ÿéš›ã«é¸ã°ã‚ŒãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼: {actual_selected}")
        
        # äºˆæ¸¬
        predictions = recommender.predict(test_menus)
        
        print(f"\näºˆæ¸¬ã‚¹ã‚³ã‚¢ TOP 10:")
        for i, pred in enumerate(predictions[:10]):
            selected_mark = "âœ…" if pred['name'] in actual_selected else "  "
            print(f"  {i+1:2d}. {selected_mark} {pred['name'][:25]:25s} (ã‚¹ã‚³ã‚¢: {pred['score']:.3f})")
    
    print("\nâœ… å­¦ç¿’å®Œäº†ï¼")


if __name__ == '__main__':
    main()
